{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, all the necessary imports are made.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, the data is loaded, adjust path as needed.\n",
    "DATA_DIR = '/Users/larsheijnen/Thesis/data/motor/MDS-UPDRS_Part_III_21Mar2025.csv'\n",
    "data = pd.read_csv(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of features (columns) in the dataset\n",
    "print(\"Number of features:\", len(data.columns))\n",
    "\n",
    "# Display the column names\n",
    "print(\"\\nFeature names:\")\n",
    "for col in data.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "unique_patients = data['PATNO'].nunique()\n",
    "print(f\"Number of unique patients: {unique_patients}\")\n",
    "\n",
    "# Display some basic statistics about patients\n",
    "print(\"\\nPatient visit statistics:\")\n",
    "patient_visits = data.groupby('PATNO').size()\n",
    "print(f\"Average visits per patient: {patient_visits.mean():.2f}\")\n",
    "print(f\"Min visits per patient: {patient_visits.min()}\")\n",
    "print(f\"Max visits per patient: {patient_visits.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime where possible\n",
    "for col in ['INFODT', 'EXAMDT', 'ORIG_ENTRY', 'LAST_UPDATE']:\n",
    "    if col in data.columns:\n",
    "        data[col] = pd.to_datetime(data[col], errors='coerce')\n",
    "\n",
    "# Convert specific date columns\n",
    "data['EXAMDT'] = pd.to_datetime(data['EXAMDT'], errors='coerce')\n",
    "data['INFODT'] = pd.to_datetime(data['INFODT'], errors='coerce')\n",
    "\n",
    "# Fallback: fill missing EXAMDT with INFODT\n",
    "data['EXAMDT'] = data['EXAMDT'].fillna(data['INFODT'])\n",
    "\n",
    "print(\"Earliest visit:\", data['EXAMDT'].min())\n",
    "print(\"Latest visit:\", data['EXAMDT'].max())\n",
    "print(\"Median interval between visits (days):\", \n",
    "      data.sort_values(['PATNO', 'EXAMDT']).groupby('PATNO')['EXAMDT'].diff().median())\n",
    "\n",
    "\n",
    "#Check outliers or wrong values\n",
    "np3_cols = [col for col in data.columns if col.startswith('NP3') and col != 'NP3TOT']\n",
    "\n",
    "# Find out-of-range values\n",
    "out_of_range = (data[np3_cols] < 0) | (data[np3_cols] > 4)\n",
    "if out_of_range.any().any():\n",
    "    print(\"Warning: Out-of-range NP3 scores detected!\")\n",
    "    display(data.loc[out_of_range.any(axis=1), ['PATNO', 'EXAMDT'] + np3_cols])\n",
    "else:\n",
    "    print(\"All NP3 scores within expected range (0–4). Ignored values in the NP3TOT column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 101 values with NaN in NP3 columns, 101 values are placed in the data to indicate missing values.\n",
    "np3_cols = [col for col in data.columns if col.startswith('NP3') and col != 'NP3TOT']\n",
    "data[np3_cols] = data[np3_cols].replace(101, np.nan)\n",
    "# Also replace 101 in NHY column with NaN\n",
    "data['NHY'] = data['NHY'].replace(101, np.nan)\n",
    "\n",
    "\n",
    "# Find out-of-range values\n",
    "# Find and remove out-of-range values\n",
    "out_of_range = (data[np3_cols] < 0) | (data[np3_cols] > 4)\n",
    "if out_of_range.any().any():\n",
    "    print(\"Warning: Out-of-range NP3 scores detected!\")\n",
    "    print(f\"Found {out_of_range.sum().sum()} out-of-range values\")\n",
    "    # Replace out-of-range values with NaN\n",
    "    data.loc[out_of_range] = np.nan\n",
    "    print(\"Out-of-range values have been replaced with NaN\")\n",
    "else:\n",
    "    print(\"All NP3 scores within expected range (0–4). Ignored NP3TOT.\")\n",
    "\n",
    "print(f\"Remaining rows after filtering NP3TOT: {len(data)}\")\n",
    "\n",
    "# Drop rows where NP3TOT is NaN\n",
    "data = data.dropna(subset=['NP3TOT'])\n",
    "print(f\"Remaining rows after dropping NaN NP3TOT: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique patients\n",
    "unique_patients = data['PATNO'].nunique()\n",
    "print(f\"Number of unique patients: {unique_patients}\")\n",
    "\n",
    "# Display some basic statistics about patients\n",
    "print(\"\\nPatient visit statistics:\")\n",
    "patient_visits = data.groupby('PATNO').size()\n",
    "print(f\"Average visits per patient: {patient_visits.mean():.2f}\")\n",
    "print(f\"Min visits per patient: {patient_visits.min()}\")\n",
    "print(f\"Max visits per patient: {patient_visits.max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis / # Data Analysis\n",
    "\n",
    "Visualize and quantify missingness across key variables using heatmaps and summary statistics.\n",
    "\n",
    "Summarize the dataset structure, check column meanings, and display initial rows to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by PATNO and visit date (prefer EXAMDT, fallback to INFODT if EXAMDT is missing)\n",
    "#INFODT is the date of the information, EXAMDT is the date of the visit. They are most of the time the same.\n",
    "if 'EXAMDT' in data.columns:\n",
    "    data = data.sort_values(['PATNO', 'EXAMDT'])\n",
    "elif 'INFODT' in data.columns:\n",
    "    data = data.sort_values(['PATNO', 'INFODT'])\n",
    "\n",
    "# Remove post-baseline visits with EXAMDT on or before baseline for each patient. This unlikely ever happens.\n",
    "if 'EXAMDT' in data.columns and 'PATNO' in data.columns:\n",
    "    baseline_dates = data.groupby('PATNO')['EXAMDT'].transform('min')\n",
    "    # Keep only visits after baseline or the baseline itself\n",
    "    mask = (data['EXAMDT'] > baseline_dates) | (data['EXAMDT'] == baseline_dates)\n",
    "    n_before = data.shape[0]\n",
    "    data = data[mask]\n",
    "    n_after = data.shape[0]\n",
    "    print(f\"Removed {n_before - n_after} visits with EXAMDT on or before baseline (except baseline itself).\")\n",
    "\n",
    "assert pd.api.types.is_datetime64_any_dtype(data['EXAMDT'])\n",
    "\n",
    "# Create a visit number per patient\n",
    "data['VISIT_NUM'] = data.groupby('PATNO').cumcount()\n",
    "\n",
    "# Show a summary of the data.\n",
    "print(data[['PATNO', 'EXAMDT', 'EVENT_ID', 'PDSTATE', 'NP3TOT', 'VISIT_NUM']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for paired ON/OFF visits (same PATNO, EVENT_ID, different PDSTATE).\n",
    "paired = data.groupby(['PATNO', 'EVENT_ID'])['PDSTATE'].nunique()\n",
    "paired_onoff = paired[paired > 1]\n",
    "print(\"Number of ON/OFF paired visits:\", paired_onoff.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Descriptive statistics for NP3TOT, NHY, and key NP3 items\n",
    "key_items = ['NP3TOT', 'NHY', 'NP3SPCH', 'NP3FACXP', 'NP3RIGN', 'NP3GAIT', 'NP3BRADY']\n",
    "desc = data[key_items].describe()\n",
    "print(\"Descriptive statistics:\\n\", desc)\n",
    "\n",
    "# Plot: Distribution of NP3TOT\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(data['NP3TOT'].dropna(), bins=30, kde=True)\n",
    "plt.title('Distribution of NP3TOT')\n",
    "plt.xlabel('NP3TOT')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Plot: Correlation heatmap for key NP3 items\n",
    "corr_matrix = data[key_items].corr()\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix: Key NP3 Items')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix for all motor-related features as a heatmap,\n",
    "# but ignore any features related to NP3TOT log or sqrt transformations.\n",
    "\n",
    "# Define all motor-related features (based on MDS-UPDRS Part III, NP3* columns)\n",
    "# Exclude NP3TOT_log and NP3TOT_sqrt if present\n",
    "motor_features = [\n",
    "    col for col in data.columns\n",
    "    if col.startswith('NP3') and col not in ['NP3TOT_log', 'NP3TOT_sqrt']\n",
    "]\n",
    "\n",
    "# Compute correlation matrix for these features\n",
    "motor_corr = data[motor_features].corr()\n",
    "\n",
    "# Plot the correlation heatmap with improved formatting\n",
    "plt.figure(figsize=(18, 14))  # Make the figure larger for better readability\n",
    "sns.heatmap(\n",
    "    motor_corr, \n",
    "    annot=True, \n",
    "    fmt=\".2f\", \n",
    "    cmap='coolwarm', \n",
    "    vmin=-1, vmax=1, \n",
    "    annot_kws={\"size\":8},   # Smaller font for annotations\n",
    "    cbar_kws={\"shrink\": 0.8, \"aspect\": 30}\n",
    ")\n",
    "plt.title('Correlation Matrix: All Motor-Related Features (NP3*)', fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check visit intervals (in days) for a sample patient, this is important for later on.\n",
    "sample_patno = data['PATNO'].dropna().unique()[0]\n",
    "sample = data[data['PATNO'] == sample_patno].sort_values('EXAMDT')\n",
    "if sample['EXAMDT'].notna().sum() > 1:\n",
    "    sample['VISIT_DIFF_DAYS'] = sample['EXAMDT'].diff().dt.days\n",
    "    print(sample[['PATNO', 'EXAMDT', 'VISIT_DIFF_DAYS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Q–Q plot for NP3TOT\n",
    "# The Q–Q plot is used to check if the data is normally distributed.\n",
    "plt.figure(figsize=(6, 6))\n",
    "stats.probplot(data['NP3TOT'].dropna(), dist=\"norm\", plot=plt)\n",
    "plt.title('Q–Q Plot of NP3TOT')\n",
    "plt.show()\n",
    "\n",
    "# Shapiro–Wilk test\n",
    "stat, p = shapiro(data['NP3TOT'].dropna())\n",
    "print(f\"Shapiro–Wilk test for NP3TOT: W={stat:.3f}, p={p:.3g}\")\n",
    "if p < 0.05:\n",
    "    print(\"NP3TOT is NOT normally distributed (reject H0 at α=0.05).\")\n",
    "else:\n",
    "    print(\"NP3TOT is consistent with normality (fail to reject H0 at α=0.05).\")\n",
    "\n",
    "\n",
    "#The distribution of \"NP3TOT\" is not normal. It appears to be positively skewed, with a likely many low values.\n",
    "# It is important to note this, especially when performing time-series modelling, since many models assume normality.\n",
    "# Rejecting the null hypothesis, the data is not normally distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transformation: Shift to avoid log(0)\n",
    "data['NP3TOT_log'] = np.log1p(data['NP3TOT'])  # log(1 + x)\n",
    "data['NP3TOT_sqrt'] = np.sqrt(data['NP3TOT'])\n",
    "\n",
    "# Check normality again\n",
    "for col in ['NP3TOT', 'NP3TOT_log', 'NP3TOT_sqrt']:\n",
    "    stat, p = stats.shapiro(data[col].dropna())\n",
    "    print(f\"{col}: Shapiro-Wilk p-value = {p:.5f}\")\n",
    "\n",
    "    stats.probplot(data[col].dropna(), dist=\"norm\", plot=plt)\n",
    "    plt.title(f\"Q–Q plot: {col}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Log Transformed (NP3TOT_log):\n",
    "# Q-Q Plot: Some improvement, especially in tail behavior, but still clear deviation.\n",
    "# Shapiro-Wilk p-value: 0.00000 → Still not normal.\n",
    "# Observation: The stair-step effect suggests many repeated small values (possibly zeros).\n",
    "\n",
    "# Square Root Transformed (NP3TOT_sqrt):\n",
    "# Q-Q Plot: Slightly better fit in mid-range values, but tails still deviate.\n",
    "# Shapiro-Wilk p-value: 0.00000 → Still not normal.\n",
    "\n",
    "# To conclude:\n",
    "# Neither log nor square root transformation fully normalize the distribution.\n",
    "# The data likely contains a large number of zeros or near-zero values (zero-inflated), and is intrinsically non-normal.\n",
    "\n",
    "#Modeling Perspective:\n",
    "# Use models that do not assume normality, like:\n",
    "# Generalized Linear Models (GLMs) with Poisson or Negative Binomial link (if count-like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(data['VISIT_NUM'], bins=20, kde=True)\n",
    "plt.title('Distribution of Visit Numbers')\n",
    "plt.xlabel('Visit Number')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_patients = {}\n",
    "for col in key_items:\n",
    "    n_missing = data.groupby('PATNO')[col].apply(lambda x: x.isna().any()).sum()\n",
    "    pct_missing = 100 * n_missing / data['PATNO'].nunique()\n",
    "    missing_patients[col] = pct_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation of NP3TOT by visit number, up to visit 30\n",
    "mean_by_visit = data.groupby('VISIT_NUM')['NP3TOT'].mean()\n",
    "std_by_visit = data.groupby('VISIT_NUM')['NP3TOT'].std()\n",
    "\n",
    "visit_mask = mean_by_visit.index <= 30\n",
    "mean_by_visit_30 = mean_by_visit[visit_mask]\n",
    "std_by_visit_30 = std_by_visit[visit_mask]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(mean_by_visit_30.index, mean_by_visit_30.values, marker='o', color='#2874A6', label='Mean NP3TOT')\n",
    "plt.fill_between(\n",
    "    mean_by_visit_30.index,\n",
    "    mean_by_visit_30.values - std_by_visit_30.values,\n",
    "    mean_by_visit_30.values + std_by_visit_30.values,\n",
    "    color='#85C1E9', alpha=0.5, label='±1 SD'\n",
    ")\n",
    "plt.xlabel('Visit Number')\n",
    "plt.ylabel('Mean NP3TOT')\n",
    "plt.title('Mean NP3TOT by Visit Number (with SD, Visits 1–30)')\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlim(left=0.5, right=30.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Stratify by PDSTATE (ON/OFF)\n",
    "print(\"\\nNP3TOT by PDSTATE:\")\n",
    "print(data.groupby('PDSTATE')['NP3TOT'].describe())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='PDSTATE', y='NP3TOT', data=data)\n",
    "plt.title('Boxplot of NP3TOT by PDSTATE (ON/OFF)')\n",
    "plt.xlabel('PDSTATE')\n",
    "plt.ylabel('NP3TOT')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.kdeplot(data=data, x='NP3TOT', hue='PDSTATE', fill=True, common_norm=False, alpha=0.5)\n",
    "plt.title('Density Plot of NP3TOT by PDSTATE (ON/OFF)')\n",
    "plt.xlabel('NP3TOT')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# NP3TOT scores are consistently higher in the OFF state, confirming that patients exhibit more severe symptoms when not medicated.\n",
    "# This validates that PDSTATE is a strong stratifying factor, and any modeling should likely adjust for or include PDSTATE.\n",
    "# The shift in all percentiles and the higher standard deviation in OFF state supports the clinical reality of fluctuating symptom severity with medication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Stratify by EVENT_ID (visit)\n",
    "print(\"\\nNP3TOT by EVENT_ID (first 5 visits):\")\n",
    "print(data.groupby('EVENT_ID')['NP3TOT'].describe().head())\n",
    "\n",
    "#Variability is high across events, which aligns with Parkinson’s disease’s fluctuating nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_patnos = [3451, 3203, 4057, 3808, 3448]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 4), sharey=True)\n",
    "for ax, patno in zip(axes, sample_patnos):\n",
    "    patient = data[(data['PATNO'] == patno) & data['EXAMDT'].notna() & data['NP3TOT'].notna()]\n",
    "    patient = patient.sort_values('EXAMDT')\n",
    "    ax.plot(patient['EXAMDT'], patient['NP3TOT'], marker='o')\n",
    "    ax.set_title(f'PATNO {patno}')\n",
    "    ax.set_xlabel('EXAMDT')\n",
    "axes[0].set_ylabel('NP3TOT')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for patno in sample_patnos:\n",
    "    patient = data[(data['PATNO'] == patno) & data['EXAMDT'].notna() & data['NP3TOT'].notna()]\n",
    "    patient = patient.sort_values('EXAMDT')\n",
    "    if len(patient) > 2:\n",
    "        # Convert EXAMDT to matplotlib date numbers for LOWESS\n",
    "        x = mdates.date2num(patient['EXAMDT'])\n",
    "        y = patient['NP3TOT'].values\n",
    "        smoothed = lowess(y, x, frac=0.5)\n",
    "        # Plot original points\n",
    "        plt.plot(patient['EXAMDT'], y, 'o', alpha=0.3)\n",
    "        # Plot smoothed line, converting x back to datetime\n",
    "        plt.plot(mdates.num2date(smoothed[:, 0]), smoothed[:, 1], label=f'PATNO {patno}')\n",
    "plt.xlabel('EXAMDT')\n",
    "plt.ylabel('NP3TOT')\n",
    "plt.title('Smoothed NP3TOT over time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Disease Progression\n",
    "\n",
    "Fit linear mixed models or other longitudinal models to estimate progression rates and account for within-patient correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate change in NP3TOT from baseline for each patient\n",
    "data['NP3TOT_BL'] = data.groupby('PATNO')['NP3TOT'].transform('first')\n",
    "data['NP3TOT_CHANGE'] = data['NP3TOT'] - data['NP3TOT_BL']\n",
    "print(\"Change from baseline (first 10 rows):\")\n",
    "print(data[['PATNO', 'EXAMDT', 'NP3TOT', 'NP3TOT_BL', 'NP3TOT_CHANGE']].head(10))\n",
    "\n",
    "#Allows tracking disease progression relative to baseline, rather than absolute scores.\n",
    "# Helps in:\n",
    "    # Longitudinal plotting (e.g., spaghetti plots or smoothed trends).\n",
    "    # Modeling change as an outcome (e.g., mixed-effects models).\n",
    "    # Identifying responders vs. non-responders to treatment/intervention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 2. Model trajectories: Linear mixed model (NP3TOT ~ time)\n",
    "# Only use rows with valid EXAMDT and NP3TOT\n",
    "model_data = data.dropna(subset=['EXAMDT', 'NP3TOT'])\n",
    "model_data['DAYS_SINCE_BL'] = (model_data['EXAMDT'] - model_data.groupby('PATNO')['EXAMDT'].transform('min')).dt.days\n",
    "md = smf.mixedlm(\"NP3TOT ~ DAYS_SINCE_BL\", model_data, groups=model_data[\"PATNO\"])\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())\n",
    "\n",
    "# Plot observed NP3TOT vs. DAYS_SINCE_BL with model prediction line\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(model_data['DAYS_SINCE_BL'], model_data['NP3TOT'], alpha=0.1, label='Observed', s=10)\n",
    "\n",
    "# Predicted line (fixed effects only)\n",
    "x_pred = np.linspace(model_data['DAYS_SINCE_BL'].min(), model_data['DAYS_SINCE_BL'].max(), 200)\n",
    "y_pred = mdf.params['Intercept'] + mdf.params['DAYS_SINCE_BL'] * x_pred\n",
    "plt.plot(x_pred, y_pred, color='red', linewidth=2, label='Model Prediction')\n",
    "\n",
    "plt.xlabel('Days Since Baseline')\n",
    "plt.ylabel('NP3TOT')\n",
    "plt.title('Linear Mixed Model: NP3TOT vs. Time')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# — Cell: Build DAYS_SINCE_BL and verify PDSTATE —\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Inspect what you’ve got\n",
    "print(\"Columns in `data`:\", data.columns.tolist())\n",
    "\n",
    "# 2. Parse your exam-date (replace 'EXAMDT' if your date column is named differently)\n",
    "data['EXAMDT'] = pd.to_datetime(data['EXAMDT'])\n",
    "\n",
    "# 3. Compute each patient’s baseline date\n",
    "data['baseline_date'] = data.groupby('PATNO')['EXAMDT'].transform('min')\n",
    "\n",
    "# 4. Days since baseline\n",
    "data['DAYS_SINCE_BL'] = (data['EXAMDT'] - data['baseline_date']).dt.days\n",
    "\n",
    "# 5. Check your PD-state column\n",
    "print(\"Unique PDSTATE values:\", data['PDSTATE'].unique())\n",
    "\n",
    "# 6. (Optional) If your column is named differently, rename it:\n",
    "# data.rename(columns={'YourStateCol':'PDSTATE'}, inplace=True)\n",
    "\n",
    "# 7. Now filter to OFF visits and drop any remaining NaNs\n",
    "df = (\n",
    "    data\n",
    "    .query(\"PDSTATE == 'OFF'\")\n",
    "    .dropna(subset=['NP3TOT','DAYS_SINCE_BL','PATNO'])\n",
    "    .assign(\n",
    "        PATNO = lambda d: d['PATNO'].astype('category'),\n",
    "        PDSTATE = lambda d: d['PDSTATE'].astype('category')\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Prepared df shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    data\n",
    "    .dropna(subset=['NP3TOT', 'DAYS_SINCE_BL', 'PATNO', 'PDSTATE'])\n",
    "    .assign(\n",
    "        # categorical grouping & state\n",
    "        PATNO        = lambda d: d['PATNO'].astype('category'),\n",
    "        PDSTATE      = lambda d: d['PDSTATE'].astype('category'),\n",
    "        # quadratic time\n",
    "        DAYS_SINCE_BL2 = lambda d: d['DAYS_SINCE_BL'] ** 2\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Prepared df:\", df.shape, \"rows\")\n",
    "\n",
    "# 3. Fit GEE with Negative Binomial family\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.genmod.families import NegativeBinomial\n",
    "from statsmodels.genmod.cov_struct import Exchangeable\n",
    "\n",
    "formula = \"NP3TOT ~ DAYS_SINCE_BL + DAYS_SINCE_BL2 + PDSTATE\"\n",
    "\n",
    "gee_pd = smf.gee(\n",
    "    formula,\n",
    "    groups=df[\"PATNO\"],\n",
    "    data=df,\n",
    "    family=NegativeBinomial(),\n",
    "    cov_struct=Exchangeable()\n",
    ").fit()\n",
    "\n",
    "print(gee_pd.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pearson residuals and fitted means from gee_pd\n",
    "resid = gee_pd.resid_pearson\n",
    "fitted = gee_pd.fittedvalues\n",
    "\n",
    "plt.scatter(fitted, resid, alpha=0.3)\n",
    "plt.axhline(0, color='black', lw=1)\n",
    "plt.xlabel(\"Fitted mean NP3TOT\")\n",
    "plt.ylabel(\"Pearson residual\")\n",
    "plt.title(\"Residuals vs. Fitted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# 2a. Fit an NB–GLM to estimate alpha\n",
    "nb_glm = sm.GLM.from_formula(\n",
    "    \"NP3TOT ~ DAYS_SINCE_BL + DAYS_SINCE_BL2 + PDSTATE\",\n",
    "    data=df,\n",
    "    family=sm.families.NegativeBinomial()\n",
    ").fit()\n",
    "\n",
    "print(\"Estimated α (dispersion) =\", nb_glm.scale)\n",
    "\n",
    "# 2b. Re-run GEE with that α\n",
    "from statsmodels.genmod.families.family import NegativeBinomial as NBfam\n",
    "\n",
    "fam = NBfam(alpha=nb_glm.scale)\n",
    "gee_refit = smf.gee(\n",
    "    \"NP3TOT ~ DAYS_SINCE_BL + DAYS_SINCE_BL2 + PDSTATE\",\n",
    "    groups=df[\"PATNO\"],\n",
    "    data=df,\n",
    "    family=fam,\n",
    "    cov_struct=Exchangeable()\n",
    ").fit()\n",
    "\n",
    "print(gee_refit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Treatment effect: ON vs. OFF within-patient (paired analysis)\n",
    "paired_idx = data.groupby(['PATNO', 'EVENT_ID'])['PDSTATE'].transform('nunique') > 1\n",
    "paired_data = data[paired_idx]\n",
    "on_scores = paired_data[paired_data['PDSTATE'] == 'ON'].set_index(['PATNO', 'EVENT_ID'])\n",
    "off_scores = paired_data[paired_data['PDSTATE'] == 'OFF'].set_index(['PATNO', 'EVENT_ID'])\n",
    "common_idx = on_scores.index.intersection(off_scores.index)\n",
    "np3tot_diff = on_scores.loc[common_idx, 'NP3TOT'] - off_scores.loc[common_idx, 'NP3TOT']\n",
    "print(\"\\nON - OFF NP3TOT difference (summary):\")\n",
    "print(np3tot_diff.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "\n",
    "# Paired t-test\n",
    "t, p_t = ttest_rel(on_scores['NP3TOT'], off_scores['NP3TOT'])\n",
    "print(f\"Paired t-test ON vs. OFF: t={t:.2f}, p={p_t:.3g}\")\n",
    "\n",
    "# Wilcoxon signed-rank test (non-parametric)\n",
    "w, p_w = wilcoxon(on_scores['NP3TOT'], off_scores['NP3TOT'])\n",
    "print(f\"Wilcoxon signed-rank test ON vs. OFF: W={w:.2f}, p={p_w:.3g}\")\n",
    "\n",
    "if p_t < 0.05:\n",
    "    print(\"Significant difference between ON and OFF (paired t-test).\")\n",
    "if p_w < 0.05:\n",
    "    print(\"Significant difference between ON and OFF (Wilcoxon).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compare treated vs. untreated over time (PDTRTMNT)\n",
    "if 'PDTRTMNT' in data.columns:\n",
    "    treated = data[data['PDTRTMNT'] == 1.0]\n",
    "    untreated = data[data['PDTRTMNT'] == 0.0]\n",
    "    print(\"\\nMean NP3TOT by visit (treated):\")\n",
    "    print(treated.groupby('VISIT_NUM')['NP3TOT'].mean().head())\n",
    "    print(\"\\nMean NP3TOT by visit (untreated):\")\n",
    "    print(untreated.groupby('VISIT_NUM')['NP3TOT'].mean().head())\n",
    "\n",
    "    # Calculate change from baseline for each patient\n",
    "    data['NP3TOT_BL'] = data.groupby('PATNO')['NP3TOT'].transform('first')\n",
    "    data['NP3TOT_CHANGE'] = data['NP3TOT'] - data['NP3TOT_BL']\n",
    "\n",
    "    # Plot mean change from baseline by group\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    treated_change = data[data['PDTRTMNT'] == 1.0].groupby('VISIT_NUM')['NP3TOT_CHANGE'].mean()\n",
    "    untreated_change = data[data['PDTRTMNT'] == 0.0].groupby('VISIT_NUM')['NP3TOT_CHANGE'].mean()\n",
    "    plt.plot(treated_change, marker='o', label='Treated')\n",
    "    plt.plot(untreated_change, marker='o', label='Untreated')\n",
    "    plt.xlabel('Visit Number')\n",
    "    plt.ylabel('Mean Change in NP3TOT from Baseline')\n",
    "    plt.title('Mean Change in NP3TOT by Visit: Treated vs Untreated')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Progression rates: Annualized change in NP3TOT, NHY\n",
    "def annualized_change(df, score):\n",
    "    df = df.sort_values('EXAMDT')\n",
    "    if len(df) < 2:\n",
    "        return np.nan\n",
    "    days = (df['EXAMDT'].iloc[-1] - df['EXAMDT'].iloc[0]).days\n",
    "    if days == 0:\n",
    "        return np.nan\n",
    "    return (df[score].iloc[-1] - df[score].iloc[0]) / (days / 365.25)\n",
    "\n",
    "annual_np3tot = data.dropna(subset=['EXAMDT', 'NP3TOT']).groupby('PATNO').apply(annualized_change, 'NP3TOT')\n",
    "print(\"\\nAnnualized NP3TOT change (summary):\")\n",
    "print(annual_np3tot.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "data['PDSTATE'] = data['PDSTATE'].astype('category')\n",
    "for state in data['PDSTATE'].cat.categories:\n",
    "    state_data = data[data['PDSTATE'] == state]\n",
    "    mean_trend = state_data.groupby('VISIT_NUM')['NP3TOT'].mean()\n",
    "    plt.plot(mean_trend.index, mean_trend.values, marker='o', label=f'{state} Mean')\n",
    "plt.xlabel('Visit Number')\n",
    "plt.ylabel('Mean NP3TOT')\n",
    "plt.title('Mean NP3TOT by Visit Number and PDSTATE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 1. Clear Separation Between ON and OFF States\n",
    "# -------------------------------------------\n",
    "# • OFF state (blue): Higher NP3TOT scores consistently across all visits\n",
    "# • ON state (orange): Lower NP3TOT scores throughout, indicating symptom improvement with medication\n",
    "\n",
    "# 2. Temporal Trends\n",
    "# ----------------\n",
    "# • Both groups show a gradual increase in NP3TOT over time, reflecting disease progression\n",
    "# • Rate of increase is steeper in the OFF state, showing more pronounced worsening when unmedicated\n",
    "\n",
    "# 3. Late-Stage Volatility (Visits ~30–34)\n",
    "# --------------------------------------\n",
    "# • Both curves become more erratic near the end (especially Visit 34)\n",
    "# • Likely causes:\n",
    "#   - Small sample sizes (dropout attrition over time)\n",
    "#   - Increased heterogeneity in late disease stages\n",
    "#   - Data artifacts (Visit 34 shows dramatic jump in ON state)\n",
    "\n",
    "# Key Insights\n",
    "# -------------\n",
    "# • Medication (ON state) consistently reduces symptom burden, but doesn't stop progression\n",
    "# • Gap between ON and OFF widens slightly over time, suggesting increasing reliance on medication\n",
    "# • End-point spikes should be examined closely for possible outliers or data sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Boxplots/violin plots: NP3TOT by visit and PDSTATE\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(x='VISIT_NUM', y='NP3TOT', hue='PDSTATE', data=data, showfliers=False)\n",
    "plt.title('Boxplot: NP3TOT by Visit and PDSTATE')\n",
    "plt.xlabel('Visit Number')\n",
    "plt.ylabel('NP3TOT')\n",
    "plt.legend(title='PDSTATE')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.violinplot(x='VISIT_NUM', y='NP3TOT', hue='PDSTATE', data=data, split=True, inner='quartile')\n",
    "plt.title('Violin Plot: NP3TOT by Visit and PDSTATE')\n",
    "plt.xlabel('Visit Number')\n",
    "plt.ylabel('NP3TOT')\n",
    "plt.legend(title='PDSTATE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Heatmap: Correlation matrix for key NP3 items\n",
    "corr_matrix = data[key_items].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix: Key NP3 Items')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_2 = '/Users/larsheijnen/Thesis/data/motor/MDS-UPDRS_Part_III_21Mar2025.csv'\n",
    "machine_learning_data = pd.read_csv(DATA_DIR_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime where possible\n",
    "for col in ['INFODT', 'EXAMDT', 'ORIG_ENTRY', 'LAST_UPDATE']:\n",
    "    if col in machine_learning_data.columns:\n",
    "        machine_learning_data[col] = pd.to_datetime(machine_learning_data[col], errors='coerce')\n",
    "\n",
    "# Convert specific date columns\n",
    "machine_learning_data['EXAMDT'] = pd.to_datetime(machine_learning_data['EXAMDT'], errors='coerce')\n",
    "machine_learning_data['INFODT'] = pd.to_datetime(machine_learning_data['INFODT'], errors='coerce')\n",
    "\n",
    "# Fallback: fill missing EXAMDT with INFODT\n",
    "machine_learning_data['EXAMDT'] = machine_learning_data['EXAMDT'].fillna(machine_learning_data['INFODT'])\n",
    "\n",
    "# Replace 101 values with NaN in NP3 columns\n",
    "np3_cols = [col for col in machine_learning_data.columns if col.startswith('NP3') and col != 'NP3TOT']\n",
    "machine_learning_data[np3_cols] = machine_learning_data[np3_cols].replace(101, np.nan)\n",
    "\n",
    "#Check outliers or wrong values\n",
    "np3_cols = [col for col in machine_learning_data.columns if col.startswith('NP3') and col != 'NP3TOT']\n",
    "\n",
    "# Find out-of-range values\n",
    "out_of_range = (machine_learning_data[np3_cols] < 0) | (machine_learning_data[np3_cols] > 4)\n",
    "if out_of_range.any().any():\n",
    "    print(\"Warning: Out-of-range NP3 scores detected!\")\n",
    "    display(machine_learning_data.loc[out_of_range.any(axis=1), ['PATNO', 'EXAMDT'] + np3_cols])\n",
    "else:\n",
    "    print(\"All NP3 scores within expected range (0–4). Ignored NP3TOT.\")\n",
    "\n",
    "# Drop rows where NP3TOT is NaN\n",
    "machine_learning_data = machine_learning_data.dropna(subset=['NP3TOT'])\n",
    "print(f\"Remaining rows after dropping NaN NP3TOT: {len(machine_learning_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_features = [\n",
    "    'PATNO', 'EVENT_ID', 'INFODT',  # Identifying information\n",
    "\n",
    "    # MDS-UPDRS Part III scores\n",
    "    'NP3SPCH', 'NP3FACXP',\n",
    "    'NP3RIGN', 'NP3RIGRU', 'NP3RIGLU', 'NP3RIGRL', 'NP3RIGLL',  # Rigidity\n",
    "    'NP3FTAPR', 'NP3FTAPL',  # Finger tapping\n",
    "    'NP3HMOVR', 'NP3HMOVL',  # Hand movements\n",
    "    'NP3PRSPR', 'NP3PRSPL',  # Pronation-supination\n",
    "    'NP3TTAPR', 'NP3TTAPL',  # Toe tapping\n",
    "    'NP3LGAGR', 'NP3LGAGL',  # Leg agility\n",
    "    'NP3RISNG',  # Arising from chair\n",
    "    'NP3GAIT', 'NP3FRZGT',  # Gait and freezing\n",
    "    'NP3PSTBL',  # Postural stability\n",
    "    'NP3POSTR',  # Posture\n",
    "    'NP3BRADY',  # Body bradykinesia\n",
    "    'NP3PTRMR', 'NP3PTRML',  # Postural tremor\n",
    "    'NP3KTRMR', 'NP3KTRML',  # Kinetic tremor\n",
    "    'NP3RTARU', 'NP3RTALU', 'NP3RTARL', 'NP3RTALL', 'NP3RTALJ', 'NP3RTCON',  # Rest tremor\n",
    "    'NP3TOT'  # Total score (needed for target creation)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant columns available in the dataset\n",
    "motor_df = machine_learning_data[[col for col in motor_features if col in machine_learning_data.columns]].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "print(f\"Initial motor assessment DataFrame shape: {motor_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expected components of NP3TOT based on MDS-UPDRS guidelines\n",
    "# These are the column names as they appear in your CSV/DataFrame\n",
    "expected_np3_components = [\n",
    "    'NP3SPCH',  # 3.1\n",
    "    'NP3FACXP', # 3.2\n",
    "    'NP3RIGN',  # 3.3a\n",
    "    'NP3RIGRU', # 3.3b\n",
    "    'NP3RIGLU', # 3.3c\n",
    "    'NP3RIGRL', # 3.3d\n",
    "    'NP3RIGLL', # 3.3e\n",
    "    'NP3FTAPR', # 3.4a\n",
    "    'NP3FTAPL', # 3.4b\n",
    "    'NP3HMOVR', # 3.5a\n",
    "    'NP3HMOVL', # 3.5b\n",
    "    'NP3PRSPR', # 3.6a\n",
    "    'NP3PRSPL', # 3.6b\n",
    "    'NP3TTAPR', # 3.7a\n",
    "    'NP3TTAPL', # 3.7b\n",
    "    'NP3LGAGR', # 3.8a\n",
    "    'NP3LGAGL', # 3.8b\n",
    "    'NP3RISNG', # 3.9\n",
    "    'NP3GAIT',  # 3.10\n",
    "    'NP3FRZGT', # 3.11\n",
    "    'NP3PSTBL', # 3.12\n",
    "    'NP3POSTR', # 3.13\n",
    "    'NP3BRADY', # 3.14\n",
    "    'NP3PTRMR', # 3.15a (Postural Tremor Right Hand)\n",
    "    'NP3PTRML', # 3.15b (Postural Tremor Left Hand)\n",
    "    'NP3KTRMR', # 3.16a (Kinetic Tremor Right Hand)\n",
    "    'NP3KTRML', # 3.16b (Kinetic Tremor Left Hand)\n",
    "    'NP3RTARU', # 3.17a (Rest Tremor Right Upper Extremity)\n",
    "    'NP3RTALU', # 3.17b (Rest Tremor Left Upper Extremity)\n",
    "    'NP3RTARL', # 3.17c (Rest Tremor Right Lower Extremity)\n",
    "    'NP3RTALL', # 3.17d (Rest Tremor Left Lower Extremity)\n",
    "    'NP3RTALJ', # 3.17e (Rest Tremor Jaw)\n",
    "    'NP3RTCON'  # 3.18 (Rest Tremor Constancy)\n",
    "]\n",
    "\n",
    "# motor_features is defined in a previous cell.\n",
    "# Let's find which features in motor_features are part of the expected NP3 components\n",
    "actual_np3_sum_components_in_motor_features = [col for col in motor_features if col in expected_np3_components]\n",
    "\n",
    "print(f\"Number of expected NP3 components: {len(expected_np3_components)}\")\n",
    "print(f\"Expected NP3 components: {expected_np3_components}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Number of NP3 sum components found in your 'motor_features' list: {len(actual_np3_sum_components_in_motor_features)}\")\n",
    "print(f\"Actual NP3 sum components from 'motor_features': {actual_np3_sum_components_in_motor_features}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Verify the sum for a few rows in the original motor_df (before any dropping of NaNs for specific columns if that happens later)\n",
    "# Make sure motor_df is loaded and NP3TOT is present\n",
    "if 'motor_df' in locals() and 'NP3TOT' in motor_df.columns:\n",
    "    # Calculate the sum of these components for each row\n",
    "    motor_df['Calculated_NP3TOT'] = motor_df[actual_np3_sum_components_in_motor_features].sum(axis=1)\n",
    "\n",
    "    # Compare with the original NP3TOT\n",
    "    # We'll check the first 5 rows and a few random rows where NP3TOT is not 0 (if any)\n",
    "    comparison_df = motor_df[['NP3TOT', 'Calculated_NP3TOT']].copy()\n",
    "    comparison_df['Difference'] = comparison_df['NP3TOT'] - comparison_df['Calculated_NP3TOT']\n",
    "\n",
    "    print(\"Comparison of NP3TOT with sum of its identified components (first 5 rows):\")\n",
    "    print(comparison_df.head())\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Check if all differences are zero (or very close to zero, allowing for potential float precision issues if data was manipulated)\n",
    "    if comparison_df['Difference'].abs().sum() < 1e-5: # A small tolerance for floating point arithmetic\n",
    "        print(\"Verification successful: NP3TOT matches the sum of the identified components for all rows.\")\n",
    "    else:\n",
    "        print(\"Verification FAILED: NP3TOT does NOT match the sum of the identified components for all rows.\")\n",
    "        print(f\"Number of rows with mismatch: {len(comparison_df[comparison_df['Difference'].abs() > 1e-5])}\")\n",
    "        print(\"Rows with discrepancies:\")\n",
    "        print(comparison_df[comparison_df['Difference'].abs() > 1e-5].head())\n",
    "\n",
    "    # It's good practice to drop the temporary column if you don't need it later\n",
    "    # motor_df.drop(columns=['Calculated_NP3TOT'], inplace=True)\n",
    "else:\n",
    "    print(\"motor_df not found or NP3TOT column is missing. Cannot perform sum verification.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in motor_df BEFORE any dropna (immediately after creation):\")\n",
    "print(f\"Shape of motor_df: {motor_df.shape}\")\n",
    "print(\"\\nNaNs per column in motor_features + NP3TOT:\")\n",
    "nan_counts = motor_df[motor_features + ['NP3TOT']].isnull().sum()\n",
    "print(nan_counts[nan_counts > 0]) # Only print columns that have NaNs\n",
    "\n",
    "print(f\"\\nTotal rows with any NaN in motor_features: {motor_df[motor_features].isnull().any(axis=1).sum()}\")\n",
    "print(f\"Total rows with NaN in NP3TOT: {motor_df['NP3TOT'].isnull().sum()}\")\n",
    "\n",
    "# Display a few rows that have NaNs in any of the motor_feature columns for context\n",
    "if motor_df[motor_features].isnull().any(axis=1).sum() > 0:\n",
    "    print(\"\\nExample rows with NaNs in motor_features:\")\n",
    "    print(motor_df[motor_df[motor_features].isnull().any(axis=1)].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_df.sort_values(['PATNO', 'INFODT'], inplace=True)\n",
    "\n",
    "motor_df['days_since_baseline'] = motor_df.groupby('PATNO')['INFODT'].transform(lambda x: (x - x.min()).dt.days)\n",
    "\n",
    "# --- Create the Target Variable: NP3TOT of the *next* visit ---\n",
    "motor_df['NP3TOT_next_visit'] = motor_df.groupby('PATNO')['NP3TOT'].shift(-1)\n",
    "\n",
    "# --- Create Lag Features (Example: Previous NP3TOT) ---\n",
    "# This demonstrates adding explicit time features. Can add more lags or differences.\n",
    "motor_df['NP3TOT_lag1'] = motor_df.groupby('PATNO')['NP3TOT'].shift(1)\n",
    "motor_df['NP3TOT_diff1'] = motor_df.groupby('PATNO')['NP3TOT'].diff(1)\n",
    "# Add lag 2 features\n",
    "motor_df['NP3TOT_lag2'] = motor_df.groupby('PATNO')['NP3TOT'].shift(2)\n",
    "motor_df['NP3TOT_diff2'] = motor_df.groupby('PATNO')['NP3TOT'].diff(2)\n",
    "# Add lag 3 features\n",
    "motor_df['NP3TOT_lag3'] = motor_df.groupby('PATNO')['NP3TOT'].shift(3)\n",
    "motor_df['NP3TOT_diff3'] = motor_df.groupby('PATNO')['NP3TOT'].diff(3)\n",
    "\n",
    "motor_df['days_since_prev1'] = motor_df.groupby('PATNO')['INFODT'].diff(1).dt.days\n",
    "motor_df['days_since_prev2'] = motor_df.groupby('PATNO')['INFODT'].diff(2).dt.days\n",
    "motor_df['days_since_prev3'] = motor_df.groupby('PATNO')['INFODT'].diff(3).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where the target ('NP3TOT_next_visit') is NaN (these are the last visits per patient)\n",
    "# Also drop rows where lag features are NaN (first visits)\n",
    "progression_df = motor_df.dropna(subset=[\n",
    "    'NP3TOT_next_visit', \n",
    "    'NP3TOT_lag1', 'NP3TOT_diff1', \n",
    "    'NP3TOT_lag2', 'NP3TOT_diff2',\n",
    "    'NP3TOT_lag3', 'NP3TOT_diff3',\n",
    "    'days_since_prev1', 'days_since_prev2', 'days_since_prev3'\n",
    "]).copy()\n",
    "\n",
    "print(f\"\\nShape after creating target and lag features & dropping NaNs: {progression_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original selection of NP3 items (current visit)\n",
    "individual_np3_items = [col for col in progression_df.columns if \n",
    "                        col.startswith('NP3') and \n",
    "                        col not in ['NP3TOT', 'NP3TOT_next_visit'] and\n",
    "                        not col.startswith('NP3TOT_lag') and \n",
    "                        not col.startswith('NP3TOT_diff')]\n",
    "\n",
    "# Explicitly list all engineered features to ensure no overlap and correct selection\n",
    "engineered_features = [\n",
    "    'days_since_baseline',\n",
    "    'NP3TOT_lag1', 'NP3TOT_diff1',\n",
    "    'NP3TOT_lag2', 'NP3TOT_diff2',\n",
    "    'NP3TOT_lag3', 'NP3TOT_diff3',\n",
    "    'days_since_prev1', 'days_since_prev2', 'days_since_prev3'\n",
    "]\n",
    "\n",
    "feature_cols = individual_np3_items + engineered_features\n",
    "\n",
    "# Ensure all listed engineered_features actually exist in progression_df.columns to avoid errors\n",
    "feature_cols = [col for col in feature_cols if col in progression_df.columns]\n",
    "# Remove duplicates just in case, and maintain order\n",
    "from collections import OrderedDict\n",
    "feature_cols = list(OrderedDict.fromkeys(feature_cols))\n",
    "\n",
    "\n",
    "X = progression_df[feature_cols]\n",
    "y = progression_df['NP3TOT_next_visit'] # y definition remains the same\n",
    "groups = progression_df['PATNO'] # groups definition remains the same\n",
    "\n",
    "print(f\"\\nCorrected Feature shape (X): {X.shape}\")\n",
    "print(f\"Target shape (y): {y.shape}\")\n",
    "print(f\"Group shape: {groups.shape}\")\n",
    "print(\"\\nCorrected Features used:\")\n",
    "print(X.columns.tolist())\n",
    "print(f\"\\nNumber of features: {len(X.columns.tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups))\n",
    "\n",
    "X_train = X.iloc[train_idx]\n",
    "X_test = X.iloc[test_idx]\n",
    "y_train = y.iloc[train_idx]\n",
    "y_test = y.iloc[test_idx]\n",
    "groups_train = groups.iloc[train_idx]\n",
    "groups_test = groups.iloc[test_idx]\n",
    "\n",
    "print(f\"\\nTrain shapes: X={X_train.shape}, y={y_train.shape}, groups={groups_train.shape}\")\n",
    "print(f\"Test shapes: X={X_test.shape}, y={y_test.shape}, groups={groups_test.shape}\")\n",
    "print(f\"Number of unique patients in train: {groups_train.nunique()}\")\n",
    "print(f\"Number of unique patients in test: {groups_test.nunique()}\")\n",
    "\n",
    "# Verify no patient overlap (should be empty)\n",
    "train_patients = set(groups_train.unique())\n",
    "test_patients = set(groups_test.unique())\n",
    "print(f\"Overlap patients: {train_patients.intersection(test_patients)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler ONLY on training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both train and test data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "print(\"\\nScaled training data head:\")\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"SVR\": SVR(),\n",
    "    \"RandomForest Regressor\": RandomForestRegressor(random_state=42, n_estimators=100), # Good baseline\n",
    "    \"GradientBoosting Regressor\": GradientBoostingRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42, objective='reg:squarederror'), # Specify objective for regression\n",
    "    \"KNeighbors\": KNeighborsRegressor(),\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=42),\n",
    "    \"MLP Regressor\": MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42, early_stopping=True) # Added early stopping\n",
    "}\n",
    "\n",
    "results_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 7. Model Training and Evaluation (Single Split)\n",
    "\n",
    "# Define models to train (Added RandomForest)\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"SVR\": SVR(),\n",
    "    \"RandomForest Regressor\": RandomForestRegressor(random_state=42, n_estimators=100),\n",
    "    \"GradientBoosting Regressor\": GradientBoostingRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42, objective='reg:squarederror'),\n",
    "    \"KNeighbors\": KNeighborsRegressor(),\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=42),\n",
    "    \"MLP Regressor\": MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42, early_stopping=True)\n",
    "}\n",
    "\n",
    "# Initialize results list\n",
    "results_test = []\n",
    "\n",
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "# Train and evaluate each model on the single train/test split\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    # --- MODIFICATION START ---\n",
    "    # Convert DataFrames to NumPy arrays for fitting and prediction\n",
    "    # Ensure y_train and y_test are also Series/arrays (which they should be)\n",
    "    model.fit(X_train_scaled.values, y_train) # Use .values\n",
    "    y_pred = model.predict(X_test_scaled.values) # Use .values\n",
    "    # --- MODIFICATION END ---\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    results_test.append({\"Model\": name, \"MAE\": mae, \"RMSE\": rmse, \"R-squared\": r2})\n",
    "    print(f\"{name}: MAE={mae:.3f}, RMSE={rmse:.3f}, R2={r2:.3f}\")\n",
    "\n",
    "    # Plot actual vs predicted for the test set\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual NP3TOT (Next Visit)\")\n",
    "    plt.ylabel(\"Predicted NP3TOT (Next Visit)\")\n",
    "    plt.title(f\"Test Set: Actual vs Predicted ({name})\\nR2={r2:.3f}\")\n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--') # Add y=x line\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Write test results to a CSV file\n",
    "# csv_file_test = \"/Users/larsheijnen/Thesis/Modelling/Motor-PPMI/results_motor/model_results_progression_test_lag3.csv\"\n",
    "# try:\n",
    "#     with open(csv_file_test, mode='w', newline='') as file:\n",
    "#         writer = csv.DictWriter(file, fieldnames=[\"Model\", \"MAE\", \"RMSE\", \"R-squared\"])\n",
    "#         writer.writeheader()\n",
    "#         writer.writerows(results_test)\n",
    "#     print(f\"\\nTest set results written to {csv_file_test}\")\n",
    "# except IOError as e:\n",
    "#     print(f\"Error writing file {csv_file_test}: {e}\")\n",
    "\n",
    "# Display test results as DataFrame\n",
    "results_test_df = pd.DataFrame(results_test)\n",
    "print(\"\\nTest Set Performance Summary:\")\n",
    "print(results_test_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results list for CV\n",
    "results_cv = []\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = GroupKFold(n_splits=5) # Use GroupKFold\n",
    "\n",
    "\n",
    "print(\"\\n--- Evaluating with Cross-Validation (on Training Data) ---\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Cross-validating {name}...\")\n",
    "\n",
    "    # --- MODIFICATION START ---\n",
    "    # Get cross-validated predictions on the scaled training data (NumPy array)\n",
    "    y_pred_cv = cross_val_predict(model, X_train_scaled.values, y_train, groups=groups_train, cv=cv) # Use .values\n",
    "    # --- MODIFICATION END ---\n",
    "\n",
    "    mae_cv = mean_absolute_error(y_train, y_pred_cv)\n",
    "    rmse_cv = np.sqrt(mean_squared_error(y_train, y_pred_cv))\n",
    "    r2_cv = r2_score(y_train, y_pred_cv)\n",
    "\n",
    "    results_cv.append({\"Model\": name, \"MAE\": mae_cv, \"RMSE\": rmse_cv, \"R-squared\": r2_cv})\n",
    "    print(f\"{name}: MAE={mae_cv:.3f}, RMSE={rmse_cv:.3f}, R2={r2_cv:.3f}\")\n",
    "\n",
    "    # Plot actual vs predicted for CV\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(y_train, y_pred_cv, alpha=0.5)\n",
    "    plt.xlabel(\"Actual NP3TOT (Next Visit)\")\n",
    "    plt.ylabel(\"Predicted NP3TOT (Next Visit)\")\n",
    "    plt.title(f\"CV (Train Set): Actual vs Predicted ({name})\\nR2={r2_cv:.3f}\")\n",
    "    min_val = min(y_train.min(), y_pred_cv.min())\n",
    "    max_val = max(y_train.max(), y_pred_cv.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--') # Add y=x line\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Write CV results to a CSV file in the specified directory\n",
    "# csv_file_cv = \"/Users/larsheijnen/Thesis/Modelling/Motor-PPMI/results_motor/model_results_progression_test__cv_lag3.csv\"\n",
    "# try:\n",
    "#     with open(csv_file_cv, mode='w', newline='') as file:\n",
    "#         writer = csv.DictWriter(file, fieldnames=[\"Model\", \"MAE\", \"RMSE\", \"R-squared\"])\n",
    "#         writer.writeheader()\n",
    "#         writer.writerows(results_cv)\n",
    "#     print(f\"\\nCross-validation results written to {csv_file_cv}\")\n",
    "# except IOError as e:\n",
    "#     print(f\"Error writing file {csv_file_cv}: {e}\")\n",
    "\n",
    "# Display CV results as DataFrame\n",
    "results_cv_df = pd.DataFrame(results_cv)\n",
    "print(\"\\nCross-Validation Performance Summary (on Training Data):\")\n",
    "print(results_cv_df.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
